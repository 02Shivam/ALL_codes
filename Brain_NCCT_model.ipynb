{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(case_id):\n",
    "    image_path = f\"C:\\\\Users\\\\shiva\\\\Downloads\\\\HYPODENSITY-DATA\\\\{case_id}\\\\{case_id}_NCCT.nii.gz\"\n",
    "    mask_path = f\"C:\\\\Users\\\\shiva\\\\Downloads\\\\HYPODENSITY-DATA\\\\{case_id}\\\\{case_id}_ROI.nii.gz\"\n",
    "    #C:\\Users\\shiva\\Downloads\\HYPODENSITY-DATA\\ProxmedImg006\\ProxmedImg006_NCCT.nii.gz\n",
    "\n",
    "    image_data = nib.load(image_path).get_fdata()\n",
    "    mask_data = nib.load(mask_path).get_fdata()\n",
    "\n",
    "    # Normalize the image data\n",
    "    image_data = (image_data - np.min(image_data)) / (np.max(image_data) - np.min(image_data))\n",
    "\n",
    "    return image_data, mask_data\n",
    "\n",
    "def preprocess_data(image_data, mask_data):\n",
    "    # You can apply additional preprocessing steps here\n",
    "    return image_data, mask_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (512, 512, 58)\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "# Load the NIfTI image\n",
    "nifti_img = nib.load(r'C:\\Users\\shiva\\Downloads\\HYPODENSITY-DATA\\ProxmedImg006\\ProxmedImg006_NCCT.nii.gz')\n",
    "\n",
    "# Get the image data array\n",
    "data = nifti_img.get_fdata()\n",
    "\n",
    "# Get the dimensions of the 3D image\n",
    "dimensions = data.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(\"Dimensions:\", dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Adjust the parameters in Conv3D layers and add more layers as needed\n",
    "    model.add(layers.Conv3D(64, (3, 3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(layers.MaxPooling3D((2, 2, 2), padding='same'))\n",
    "\n",
    "    # Add more layers as needed\n",
    "    \n",
    "\n",
    "    model.add(layers.Conv3D(1, (1, 1, 1), activation='sigmoid', padding='same'))  # Assuming binary segmentation\n",
    "\n",
    "    return model\n",
    "\n",
    "# Adjust input_shape based on your actual image dimensions\n",
    "input_shape = (512, 512, 58, 1)\n",
    "model = create_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainig The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, epochs, batch_size):\n",
    "    # Compile the model before training\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, test_labels):\n",
    "    evaluation = model.evaluate(test_data, test_labels)\n",
    "    print(\"Test Loss:\", evaluation[0])\n",
    "    print(\"Test Accuracy:\", evaluation[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_data):\n",
    "    predictions = model.predict(input_data)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting All together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Assuming you have a directory containing the NCCT images and corresponding ROI masks\n",
    "data_dir = r\"C:\\Users\\shiva\\Downloads\\HYPODENSITY-DATA\"\n",
    "all_cases = os.listdir(data_dir)\n",
    "\n",
    "# Load images and masks\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "for case_id in all_cases:\n",
    "    #img_path = os.path.join(f\"C:\\\\Users\\\\shiva\\\\Downloads\\\\HYPODENSITY-DATA\\\\{case_id}\", f\"{case_id}_NCCT.nii.gz\")\n",
    "    #mask_path = os.path.join(f\"C:\\\\Users\\\\shiva\\\\Downloads\\\\HYPODENSITY-DATA\\\\{case_id}\", f\"{case_id}_ROI.nii.gz\")\n",
    "    \n",
    "    image_data, mask_data = load_data(case_id)\n",
    "    image_data, mask_data = preprocess_data(image_data, mask_data)\n",
    "\n",
    "    \n",
    "    #images.append(img_path)\n",
    "    #masks.append(mask_path)\n",
    "\n",
    "\n",
    "# Expand dimensions for the channel axis\n",
    "\n",
    "#image_data = np.expand_dims(image_data, axis=1)\n",
    "# Build the model\n",
    "model = create_model(input_shape=image_data.shape[0:])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model,image_data[0],'\\n',mask_data[0])\n",
    "# Train the model\n",
    "train_model(Sequential(),image_data, mask_data, epochs=10, batch_size=4)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "# evaluate_model(model, test_data, test_labels)\n",
    "\n",
    "# Make predictions on new data\n",
    "#new_data = load_and_preprocess_new_data(r\"C:\\Users\\shiva\\Downloads\\HYPODENSITY-DATA\\ProxmedImg014\")\n",
    "#new_data = np.expand_dims(new_data, axis=-1)\n",
    "#predictions = predict(model, new_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.01115242 0.00966543 0.00768278 ... 0.00594796 0.00718711 0.00619579]\n",
      "  [0.01115242 0.01040892 0.00768278 ... 0.00594796 0.00693928 0.00594796]\n",
      "  [0.01065675 0.01115242 0.00842627 ... 0.00619579 0.00644362 0.00570012]\n",
      "  ...\n",
      "  [0.02503098 0.0260223  0.02503098 ... 0.03172243 0.01239157 0.01412639]\n",
      "  [0.02973978 0.02230483 0.02775713 ... 0.03048327 0.01982652 0.0173482 ]\n",
      "  [0.0394052  0.01883519 0.03023544 ... 0.03048327 0.02949195 0.02651797]]\n",
      "\n",
      " [[0.00892193 0.00793061 0.00743494 ... 0.00570012 0.00619579 0.00545229]\n",
      "  [0.0086741  0.00793061 0.00718711 ... 0.00545229 0.00619579 0.00545229]\n",
      "  [0.0086741  0.00793061 0.00768278 ... 0.00570012 0.00594796 0.00570012]\n",
      "  ...\n",
      "  [0.03246592 0.01982652 0.03965304 ... 0.0267658  0.02850062 0.02726146]\n",
      "  [0.0307311  0.01685254 0.04560099 ... 0.02255266 0.02304833 0.02354399]\n",
      "  [0.02825279 0.01288724 0.04337051 ... 0.02032218 0.02032218 0.02106568]]\n",
      "\n",
      " [[0.00693928 0.00669145 0.00693928 ... 0.00570012 0.00644362 0.00619579]\n",
      "  [0.00669145 0.00693928 0.00669145 ... 0.00545229 0.00594796 0.00619579]\n",
      "  [0.00669145 0.00693928 0.00644362 ... 0.00520446 0.00594796 0.00594796]\n",
      "  ...\n",
      "  [0.02503098 0.02750929 0.02800496 ... 0.02899628 0.02924411 0.02924411]\n",
      "  [0.02304833 0.02354399 0.02379182 ... 0.03023544 0.03023544 0.03048327]\n",
      "  [0.02230483 0.022057   0.02255266 ... 0.02701363 0.02627014 0.02726146]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.00991326 0.00644362 0.00743494 ... 0.00570012 0.00619579 0.00570012]\n",
      "  [0.01040892 0.00693928 0.00718711 ... 0.00570012 0.00594796 0.00570012]\n",
      "  [0.01140025 0.00718711 0.00743494 ... 0.00545229 0.00545229 0.00570012]\n",
      "  ...\n",
      "  [0.03221809 0.04460967 0.04436183 ... 0.02775713 0.02825279 0.02825279]\n",
      "  [0.03246592 0.04039653 0.03593556 ... 0.02998761 0.02973978 0.0307311 ]\n",
      "  [0.02973978 0.03370508 0.02750929 ... 0.02874845 0.02701363 0.02874845]]\n",
      "\n",
      " [[0.00594796 0.00619579 0.00669145 ... 0.00594796 0.00669145 0.00446097]\n",
      "  [0.00520446 0.00594796 0.00619579 ... 0.00619579 0.00644362 0.00446097]\n",
      "  [0.00545229 0.00570012 0.00570012 ... 0.00619579 0.00619579 0.0047088 ]\n",
      "  ...\n",
      "  [0.2094176  0.12118959 0.05501859 ... 0.01164808 0.01164808 0.01164808]\n",
      "  [0.24089219 0.15390335 0.07707559 ... 0.01239157 0.01189591 0.01239157]\n",
      "  [0.26468401 0.18686493 0.10334572 ... 0.01288724 0.01214374 0.01313507]]\n",
      "\n",
      " [[0.00594796 0.00594796 0.00545229 ... 0.00495663 0.00619579 0.00619579]\n",
      "  [0.00594796 0.00594796 0.00570012 ... 0.00520446 0.00644362 0.00644362]\n",
      "  [0.00570012 0.00570012 0.00570012 ... 0.00545229 0.00644362 0.00644362]\n",
      "  ...\n",
      "  [0.01660471 0.01759603 0.01883519 ... 0.01164808 0.01214374 0.01263941]\n",
      "  [0.01685254 0.0173482  0.01908302 ... 0.01214374 0.01214374 0.0133829 ]\n",
      "  [0.01660471 0.01685254 0.01982652 ... 0.01263941 0.01288724 0.01363073]]] [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming you have a directory containing the NCCT images and corresponding ROI masks\n",
    "data_dir = r\"C:\\Users\\shiva\\Downloads\\HYPODENSITY-DATA\"\n",
    "all_cases = os.listdir(data_dir)\n",
    "\n",
    "# Load images and masks\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "for case_id in all_cases:\n",
    "    #img_path = os.path.join(f\"C:\\\\Users\\\\shiva\\\\Downloads\\\\HYPODENSITY-DATA\\\\{case_id}\", f\"{case_id}_NCCT.nii.gz\")\n",
    "    #mask_path = os.path.join(f\"C:\\\\Users\\\\shiva\\\\Downloads\\\\HYPODENSITY-DATA\\\\{case_id}\", f\"{case_id}_ROI.nii.gz\")\n",
    "    \n",
    "    image_data, mask_data = load_data(case_id)\n",
    "    image_data, mask_data = preprocess_data(image_data, mask_data)\n",
    "\n",
    "    \n",
    "   \n",
    "X_train, X_test, y_train, y_test = train_test_split(image_data, mask_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    # Decoder\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    up1 = layers.UpSampling2D(size=(2, 2))(conv2)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(up1)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_shape = (512,512,58)  # Replace with your image dimensions\n",
    "model = build_unet_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 512, 35)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 512, 35)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filei8ct2s2s.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 512, 35)).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "# Assuming you have X_train, X_test, y_train, and y_test with the specified shapes\n",
    "\n",
    "# Create a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(512, 35)))  # Flatten the 3D input to 2D\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification, adjust for your task\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7348 - accuracy: 0.4369\n",
      "Test Loss: 0.7347683906555176\n",
      "Test Accuracy: 0.43689319491386414\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already trained the model using the code provided in the previous response\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "evaluation_results = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Test Loss:\", evaluation_results[0])\n",
    "print(\"Test Accuracy:\", evaluation_results[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
